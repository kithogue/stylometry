{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"Initialize gensim embeddings for joined, full and pretrained, dim 100\"\"\"\n",
    "\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "def print_similar(model: KeyedVectors) -> None:\n",
    "    # todo: retrain vectors uncased!!!\n",
    "    print('Checking similar words:')\n",
    "    for word in ['przekonywujący', 'złodzieji', 'swetr', 'ludzią']:\n",
    "      most_similar = ', '.join('%s (%.2f)' % (similar, dist)\n",
    "                               for similar, dist in model.most_similar(word)[:8])\n",
    "      print('  %s -> %s' % (word, most_similar))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def save_vectors(model_path: str, vecs_path: str) -> None:\n",
    "    model = Word2Vec.load(model_path)\n",
    "    vecs = model.wv\n",
    "    vecs.save(vecs_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# save_vectors(model_path='../ckpt/joined/sg_100/sg_100_model', vecs_path='../ckpt/joined/sg_100/sg_100.kv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW model on joined dataset\n",
      "Checking similar words:\n",
      "  przekonywujący -> argumentowania (0.82), ariunbaatar (0.81), niezręczny (0.81), actn (0.81), najefektywniejszy (0.81), finezyjny (0.80), niejednoznaczny (0.80), potażnia (0.80)\n",
      "  złodzieji -> partyzantach (0.80), jakościach (0.79), gębach (0.78), aymerica (0.78), pedia­try (0.77), abecednom (0.77), redosledusuprotno (0.77), cięszkich (0.77)\n",
      "  swetr -> 23022009r (0.87), 20102008r (0.87), lubańlubawkalublinłebałódźmielnomiędzywodziemiędzyzdrojemikołajkimikoszewomiłkówmrągowomurzasichlemuszynanadolenałęczównidzicanieborówniechorzeniedzicanowa (0.86), niepiarżysty (0.86), അസ്‌ലഹ് (0.86), 手机 (0.86), f8f (0.86), nieurodzony (0.86)\n",
      "  ludzią -> mhzzasilanie (0.67), ka¿dej (0.67), cospie (0.66), cigu (0.66), orazbeatelmania (0.66), trpimirowiczów (0.65), 140220 (0.65), duszego (0.65)\n",
      "--- 0.6775708198547363 seconds ---\n",
      "SkipGram model on joined dataset\n",
      "Checking similar words:\n",
      "  przekonywujący -> kumają (0.95), śmiertelnikom (0.95), nieskromna (0.95), myslacych (0.95), heavymetal (0.95), dupsku (0.95), swo­ją (0.95), czytujesz (0.95)\n",
      "  złodzieji -> kolesi (0.92), durniów (0.91), zwyrodnialców (0.91), uśmierzyli (0.91), małolatów (0.91), wpuścili (0.91), bando (0.91), zaniosą (0.91)\n",
      "  swetr -> spodenkijeansowe (0.96), 4xl5xl (0.96), pr01tv (0.96), 1000138568 (0.95), lezbo (0.95), alstar (0.95), howlity (0.95), niebieskinavy (0.95)\n",
      "  ludzią -> trcohe (0.95), męczyliśmy (0.95), ocenicie (0.95), marnujemy (0.95), zmarnowałeś (0.95), twiter (0.95), napierdalał (0.94), zebrałaś (0.94)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# joined_cbow_model = Word2Vec.load('../ckpt/joined/cbow_100/cbow_100_model')\n",
    "# cbow_vectors = joined_cbow_model.wv\n",
    "# cbow_vectors.save('../ckpt/joined/cbow_100/cbow_100_vectors.kv')\n",
    "joined_cbow_model = KeyedVectors.load('../ckpt/joined/cbow_100/cbow_100_vectors.kv')\n",
    "print(\"CBOW model on joined dataset\")\n",
    "start_time1 = time.time()\n",
    "print_similar(joined_cbow_model)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time1))\n",
    "# print()\n",
    "joined_skipgram_model = KeyedVectors.load('../ckpt/joined/sg_100/sg_100.kv')\n",
    "print(\"SkipGram model on joined dataset\")\n",
    "print_similar(joined_skipgram_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def word2idx(word, model):\n",
    "  return model.key_to_index[word]\n",
    "\n",
    "\n",
    "def idx2word(idx, model):\n",
    "  return model.index_to_key[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_ds_to_df(filepath: str) -> pd.DataFrame:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    label_mapping = {'label__z_zero': 0, 'label__z_amb': 2, 'label__z_minus_m': 3, 'label__z_plus_m': 1}\n",
    "    test_dict = {}\n",
    "    for line in lines:\n",
    "        line_spl = line.split(' __')\n",
    "        test_dict[line_spl[0].strip()] = line_spl[1].strip()\n",
    "    test_dict = {k:label_mapping[v] for (k, v) in test_dict.items()}\n",
    "    df = pd.DataFrame.from_dict(test_dict, orient='index').reset_index()\n",
    "    return df.rename(columns={'index': 'text', 0: 'label'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  label\n0  Potrafi przyczepic sie do wszystkie - osie sym...      3\n1  Lokalizacja w centrum - przy głównym deptaku ,...      1\n2  Komórki te odpowiadają za resorpcję kości , cz...      0\n3  Jedzenie było smaczne poza tak jak pisał em wc...      2\n4  Po zamianie leku na inny kaszel natychmiast us...      2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Potrafi przyczepic sie do wszystkie - osie sym...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Lokalizacja w centrum - przy głównym deptaku ,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Komórki te odpowiadają za resorpcję kości , cz...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Jedzenie było smaczne poza tak jak pisał em wc...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Po zamianie leku na inny kaszel natychmiast us...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_test = '../data/emb_test_data/all.sentence.test.txt'\n",
    "test = read_ds_to_df(path_to_test)\n",
    "test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  label\n0             Moim zdaniem jest wart jakieś 1100zł .      3\n1  co jest masakra szczególnie jak na początku ch...      3\n2             Poza tym krzesło ma bardzo duże luzy .      3\n3  Ścielenie łóżek , sprzątanie sal po organizowa...      0\n4  Parę niższych osób siedziało i było im wygodnie .      2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Moim zdaniem jest wart jakieś 1100zł .</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>co jest masakra szczególnie jak na początku ch...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Poza tym krzesło ma bardzo duże luzy .</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ścielenie łóżek , sprzątanie sal po organizowa...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Parę niższych osób siedziało i było im wygodnie .</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_dev = '../data/emb_test_data/all.sentence.dev.txt'\n",
    "dev = read_ds_to_df(path_to_dev)\n",
    "dev.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  label\n0  Taksowak przyjezdza na czas , wydaje sie , ze ...      0\n1  Do tej pani trafili śmy z naszą 2 - miesięczna...      0\n2  Wyróżnikiem , a także ciekawym elementem całeg...      0\n3  W zależności od pory dnia , w naszych organizm...      0\n4  Polecam wszystkim , zwłaszcza tym , dla któryc...      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Taksowak przyjezdza na czas , wydaje sie , ze ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Do tej pani trafili śmy z naszą 2 - miesięczna...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Wyróżnikiem , a także ciekawym elementem całeg...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>W zależności od pory dnia , w naszych organizm...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Polecam wszystkim , zwłaszcza tym , dla któryc...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_train = '../data/emb_test_data/all.sentence.train.txt'\n",
    "train = read_ds_to_df(path_to_train)\n",
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "count    45935.000000\nmean       100.884010\nstd         77.040292\nmin          1.000000\n25%         50.000000\n50%         83.000000\n75%        129.000000\nmax       1078.000000\ndtype: float64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "reviews_len = [len(x) for x in train['text']]\n",
    "pd.Series(reviews_len).hist()\n",
    "plt.show()\n",
    "pd.Series(reviews_len).describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def remove_outliers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    mask = (df['text'].apply(lambda x: 20 < len(str(x).split(' ')) <= 250))\n",
    "    return df.loc[mask]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "\"\"\"Ignoring unknown tokens, as they might be creating additional noise to the model performance\"\"\"\n",
    "\n",
    "def get_tokenized_x(df: pd.DataFrame, model: KeyedVectors) -> List[List[str]]:\n",
    "    X = df['text'].tolist()\n",
    "    X = [list(tokenize(sentence)) for sentence in X]\n",
    "    result = []\n",
    "    for sentence in X:\n",
    "        indices = []\n",
    "        for word in sentence:\n",
    "            if word.lower() in list(model.index_to_key):\n",
    "                indices.append(word2idx(word.lower(), model))\n",
    "        result.append(indices)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45935, 2)\n",
      "(13863, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "train = remove_outliers(train)\n",
    "print(train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "dev = remove_outliers(dev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "test = remove_outliers(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    \"\"\"Class inheriting after torch dataset,\n",
    "    reads the data, encodes the input and target as tensors\n",
    "    pads them to max length\n",
    "    the overriden getitem returns a tensor tuple of input and target\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, model_path = '../ckpt/joined/cbow_100/cbow_100_vectors.kv'):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.model = KeyedVectors.load(model_path)\n",
    "        embeddings = torch.Tensor(self.model.vectors)\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(embeddings=embeddings)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def embed(self, sentence: str) -> torch.Tensor:\n",
    "        tokens = list(tokenize(sentence))\n",
    "        tokens = [word2idx(word=tok.lower(), model=self.model) for tok in tokens if tok in self.model.key_to_index.keys()]\n",
    "        tokens = torch.LongTensor(tokens)\n",
    "        embedded = self.embedding(tokens)\n",
    "        return embedded.cuda()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return self.embed(sentence), label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers: int,\n",
    "                 num_classes: int, dimension=100,\n",
    "                 model_path: str = '../ckpt/joined/cbow_100/cbow_100_vectors.kv'):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.lstm = nn.LSTM(input_size=dimension,\n",
    "                            hidden_size=dimension,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.linear = nn.Linear(dimension, num_classes)\n",
    "        self.emb_model = KeyedVectors.load(model_path)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        output, hidden_states = self.lstm(batch)\n",
    "        out_forward = output[:, -1]\n",
    "        text_features = self.dropout(out_forward)\n",
    "\n",
    "        text_logits = self.linear(text_features)\n",
    "        text_logits = text_logits[:,:]\n",
    "        probs = torch.sigmoid(text_logits)\n",
    "\n",
    "        return probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "train_ds = ClassificationDataset(train['text'].to_numpy(), train['label'].to_numpy())\n",
    "dev_ds = ClassificationDataset(dev['text'].to_numpy(), dev['label'].to_numpy())\n",
    "test_ds = ClassificationDataset(test['text'].to_numpy(), test['label'].to_numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from typing import Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def count_correct(\n",
    "    y_pred: torch.Tensor, y_true: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    preds = torch.argmax(y_pred, dim=1)\n",
    "    return (preds == y_true).float().sum()\n",
    "\n",
    "def validate(\n",
    "    model: nn.Module,\n",
    "    loss_fn: Union[torch.nn.CrossEntropyLoss, torch.nn.BCELoss],\n",
    "    dataloader: DataLoader,\n",
    "    is_binary: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    all = 0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        y_pred = model(X_batch)\n",
    "        all += len(y_pred)\n",
    "        loss += loss_fn(y_pred, y_batch.cuda()).sum()\n",
    "        correct += count_correct(y_pred, y_batch.cuda())\n",
    "    return loss / all, correct / all\n",
    "\n",
    "def fit(\n",
    "        model: nn.Module,\n",
    "        optimiser: torch.optim.Optimizer,\n",
    "        loss_fn: torch.nn.CrossEntropyLoss,\n",
    "        train_dl: DataLoader,\n",
    "        val_dl: DataLoader,\n",
    "        epochs: int,\n",
    "        print_metrics: str = True,\n",
    "):\n",
    "  for epoch in range(epochs):\n",
    "      model.train()\n",
    "      for X_batch, y_batch in tqdm(train_dl):\n",
    "          y_pred = model(X_batch)\n",
    "          loss = loss_fn(y_pred, y_batch.cuda())\n",
    "\n",
    "          loss.backward()\n",
    "          optimiser.step()\n",
    "          optimiser.zero_grad()\n",
    "\n",
    "      if print_metrics:\n",
    "          model.eval()\n",
    "          with torch.no_grad():\n",
    "              train_loss, train_acc = validate(\n",
    "                  model=model, loss_fn=loss_fn, dataloader=train_dl\n",
    "              )\n",
    "              val_loss, val_acc = validate(\n",
    "                  model=model, loss_fn=loss_fn, dataloader=val_dl\n",
    "              )\n",
    "              print(\n",
    "                  f\"Epoch {epoch}: \"\n",
    "                  f\"train loss = {train_loss:.3f} (acc: {train_acc:.3f}), \"\n",
    "                  f\"validation loss = {val_loss:.3f} (acc: {val_acc:.3f})\"\n",
    "              )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sequences = [b[0] for b in batch]\n",
    "    labels = [b[1] for b in batch]\n",
    "    sequences = sorted(sequences, key=lambda x: x.shape[0], reverse=True)\n",
    "    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "    return sequences_padded, torch.stack(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "model_CBOW = LSTM(num_layers=2, num_classes=4).cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_CBOW.parameters(), lr=0.001)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl = DataLoader(dev_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Tensor"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(val_dl))\n",
    "type(batch[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 9.2296e-01,  5.4049e-01, -1.7758e-01,  ..., -1.5364e-01,\n          -7.0579e-02,  7.3740e-01],\n         [ 4.8692e+00, -4.1691e-01,  4.6085e-01,  ..., -2.7845e-01,\n          -1.4422e+00,  1.5397e+00],\n         [ 6.0732e+00, -1.7143e+00, -1.9337e+00,  ..., -1.9084e+00,\n           3.3864e-01, -1.4694e+00],\n         ...,\n         [ 8.3325e-01, -5.7760e-01, -4.1955e-01,  ...,  2.3566e-01,\n          -2.0078e-01, -7.5610e-02],\n         [ 3.5230e-01,  3.0727e-01, -5.0487e-02,  ..., -2.5199e-01,\n          -1.0895e-01,  1.3289e-01],\n         [ 5.9534e-02,  5.4448e-01,  1.0775e+00,  ...,  9.0538e-01,\n           8.6923e-01,  2.8426e-01]],\n\n        [[ 3.6850e+00, -1.2062e+00, -2.6835e+00,  ...,  1.1382e+00,\n          -8.2648e-02, -1.4407e+00],\n         [ 5.4467e+00, -5.2929e-01, -1.1692e+00,  ...,  4.9529e-01,\n          -9.8189e-01,  3.3662e-01],\n         [ 6.1459e-01, -1.6662e-01, -3.3168e-02,  ...,  4.0492e-02,\n          -1.2076e-01,  2.7654e-01],\n         ...,\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00]],\n\n        [[ 5.1813e-01,  5.8052e-01, -3.0982e-01,  ..., -1.9057e-01,\n          -6.1481e-01,  2.8597e-01],\n         [ 2.0787e-01,  6.4557e-02,  1.6648e-01,  ...,  1.6087e-02,\n          -1.7836e-01,  1.5758e-01],\n         [ 6.1183e-02,  2.3683e-02, -4.4004e-02,  ..., -3.7561e-02,\n          -3.8240e-02,  1.7706e-02],\n         ...,\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00]],\n\n        ...,\n\n        [[ 1.7267e+00,  1.0433e+00, -1.1541e+00,  ..., -2.9463e-01,\n           1.2520e+00,  2.2865e-01],\n         [ 4.2366e-01,  8.0637e-01,  1.7050e-01,  ..., -8.6949e-01,\n          -3.9011e-02, -6.1437e-02],\n         [-7.4804e-01, -6.2730e-01, -1.3226e+00,  ..., -7.0518e-02,\n          -2.1191e+00, -3.2031e-01],\n         ...,\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00]],\n\n        [[ 9.8087e-02,  7.1151e-02,  5.0265e-02,  ...,  3.9990e-02,\n          -5.2577e-02, -2.2406e-03],\n         [ 6.0048e-01,  1.5550e+00,  9.2476e-01,  ..., -2.4076e+00,\n           1.9567e+00, -1.5895e+00],\n         [ 4.4753e+00, -4.3966e-01, -4.1331e+00,  ...,  2.3658e+00,\n           2.3755e+00, -1.3766e+00],\n         ...,\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00]],\n\n        [[ 1.0120e-01,  4.2574e-01, -5.6317e-02,  ...,  1.3547e-03,\n          -5.7567e-01,  3.0900e-01],\n         [-3.2604e-01,  6.8853e-01, -2.8490e-02,  ..., -4.7739e-01,\n           2.9684e-01,  6.5230e-01],\n         [-1.5632e-01,  2.1730e+00,  9.6727e-01,  ...,  1.2515e-01,\n           2.2095e+00, -6.1066e-01],\n         ...,\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00]]], device='cuda:0')"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 0, 1, 3, 3, 1, 3, 3, 1, 3, 3, 3, 0, 1, 1, 3, 1, 3, 2, 1, 2, 3, 0, 1,\n        0, 3, 1, 0, 0, 1, 1, 2, 1, 0, 3, 3, 1, 0, 0, 3, 2, 0, 3, 2, 1, 3, 1, 3,\n        3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 0, 0, 3, 3, 3])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([64])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:05<00:00, 41.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 0.021 (acc: 0.383), validation loss = 0.020 (acc: 0.394)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:04<00:00, 53.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 0.021 (acc: 0.383), validation loss = 0.020 (acc: 0.394)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:04<00:00, 53.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train loss = 0.021 (acc: 0.383), validation loss = 0.020 (acc: 0.394)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:04<00:00, 48.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train loss = 0.021 (acc: 0.383), validation loss = 0.021 (acc: 0.394)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:04<00:00, 52.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train loss = 0.021 (acc: 0.383), validation loss = 0.020 (acc: 0.394)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:04<00:00, 53.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train loss = 0.021 (acc: 0.383), validation loss = 0.020 (acc: 0.394)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 118/217 [00:02<00:02, 48.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [38]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_CBOW\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_dl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [31]\u001B[0m, in \u001B[0;36mfit\u001B[1;34m(model, optimiser, loss_fn, train_dl, val_dl, epochs, print_metrics)\u001B[0m\n\u001B[0;32m     42\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fn(y_pred, y_batch\u001B[38;5;241m.\u001B[39mcuda())\n\u001B[0;32m     44\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m---> 45\u001B[0m     \u001B[43moptimiser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     46\u001B[0m     optimiser\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m print_metrics:\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    138\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[1;32m--> 140\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    141\u001B[0m     obj\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     22\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 23\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     25\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(prev_grad)\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\optim\\adam.py:234\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure, grad_scaler)\u001B[0m\n\u001B[0;32m    231\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    232\u001B[0m             state_steps\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m--> 234\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    235\u001B[0m \u001B[43m         \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    236\u001B[0m \u001B[43m         \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    237\u001B[0m \u001B[43m         \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    238\u001B[0m \u001B[43m         \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    239\u001B[0m \u001B[43m         \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    240\u001B[0m \u001B[43m         \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    241\u001B[0m \u001B[43m         \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    242\u001B[0m \u001B[43m         \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    243\u001B[0m \u001B[43m         \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    244\u001B[0m \u001B[43m         \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    245\u001B[0m \u001B[43m         \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    246\u001B[0m \u001B[43m         \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    247\u001B[0m \u001B[43m         \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    248\u001B[0m \u001B[43m         \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    249\u001B[0m \u001B[43m         \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    250\u001B[0m \u001B[43m         \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    251\u001B[0m \u001B[43m         \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    252\u001B[0m \u001B[43m         \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\optim\\adam.py:300\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    297\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    298\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[1;32m--> 300\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    301\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    302\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    303\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    304\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    305\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    306\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    307\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    308\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    309\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    310\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    311\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    312\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    314\u001B[0m \u001B[43m     \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    316\u001B[0m \u001B[43m     \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\optim\\adam.py:364\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[0;32m    362\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[0;32m    363\u001B[0m exp_avg\u001B[38;5;241m.\u001B[39mmul_(beta1)\u001B[38;5;241m.\u001B[39madd_(grad, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n\u001B[1;32m--> 364\u001B[0m exp_avg_sq\u001B[38;5;241m.\u001B[39mmul_(beta2)\u001B[38;5;241m.\u001B[39maddcmul_(grad, \u001B[43mgrad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconj\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[0;32m    366\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m capturable \u001B[38;5;129;01mor\u001B[39;00m differentiable:\n\u001B[0;32m    367\u001B[0m     step \u001B[38;5;241m=\u001B[39m step_t\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "fit(model=model_CBOW,\n",
    "    optimiser=optimizer,\n",
    "    loss_fn=criterion,\n",
    "    train_dl=train_dl,\n",
    "    val_dl=val_dl,\n",
    "    epochs=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "model_SG = LSTM(num_layers=2, num_classes=4, model_path='../ckpt/joined/sg_100/sg_100.kv').cuda()\n",
    "optimizer_SG = torch.optim.Adam(model_SG.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/217 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [96]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_SG\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer_SG\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_dl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [87]\u001B[0m, in \u001B[0;36mfit\u001B[1;34m(model, optimiser, loss_fn, train_dl, val_dl, epochs, print_metrics)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m     39\u001B[0m     model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m---> 40\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m X_batch, y_batch \u001B[38;5;129;01min\u001B[39;00m tqdm(train_dl):\n\u001B[0;32m     41\u001B[0m         y_pred \u001B[38;5;241m=\u001B[39m model(X_batch)\n\u001B[0;32m     42\u001B[0m         loss \u001B[38;5;241m=\u001B[39m loss_fn(y_pred, y_batch)\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tqdm\\std.py:1195\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1192\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1194\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1195\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1196\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1197\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1198\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Input \u001B[1;32mIn [16]\u001B[0m, in \u001B[0;36mClassificationDataset.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m     31\u001B[0m label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels[index]\n\u001B[0;32m     32\u001B[0m label \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(label, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[1;32m---> 33\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentence\u001B[49m\u001B[43m)\u001B[49m, label\n",
      "Input \u001B[1;32mIn [16]\u001B[0m, in \u001B[0;36mClassificationDataset.embed\u001B[1;34m(self, sentence)\u001B[0m\n\u001B[0;32m     25\u001B[0m tokens \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mLongTensor(tokens)\n\u001B[0;32m     26\u001B[0m embedded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(tokens)\n\u001B[1;32m---> 27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43membedded\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "fit(model=model_SG,\n",
    "    optimiser=optimizer_SG,\n",
    "    loss_fn=criterion,\n",
    "    train_dl=train_dl,\n",
    "    val_dl=val_dl,\n",
    "    epochs=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from torch.nn import Linear, ReLU, Sigmoid\n",
    "from torch.nn.init import kaiming_uniform_, xavier_uniform_\n",
    "\n",
    "\n",
    "class ClassificationMLP(torch.nn.Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super().__init__()\n",
    "        self.hidden1 = Linear(n_inputs, 20)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # Second hidden layer\n",
    "        self.hidden2 = Linear(20, 10)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # Third hidden layer\n",
    "        self.hidden3 = Linear(10,1)\n",
    "        # xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.act1(x)\n",
    "        # Second hidden layer\n",
    "        x = self.hidden2(x)\n",
    "        x = self.act2(x)\n",
    "        # Third hidden layer\n",
    "        x = self.hidden3(x)\n",
    "        x = self.act3(x)\n",
    "        return torch.squeeze(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 68.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 0.059 (acc: 0.275), validation loss = 0.060 (acc: 0.288)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 71.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 0.059 (acc: 0.276), validation loss = 0.060 (acc: 0.285)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:02<00:00, 72.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train loss = 0.059 (acc: 0.278), validation loss = 0.060 (acc: 0.288)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 68.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train loss = 0.059 (acc: 0.279), validation loss = 0.060 (acc: 0.287)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:02<00:00, 72.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train loss = 0.059 (acc: 0.281), validation loss = 0.060 (acc: 0.282)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 68.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train loss = 0.059 (acc: 0.279), validation loss = 0.060 (acc: 0.285)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 71.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train loss = 0.059 (acc: 0.279), validation loss = 0.060 (acc: 0.284)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 72.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train loss = 0.059 (acc: 0.280), validation loss = 0.060 (acc: 0.287)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:02<00:00, 72.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train loss = 0.059 (acc: 0.281), validation loss = 0.060 (acc: 0.290)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 71.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train loss = 0.059 (acc: 0.279), validation loss = 0.060 (acc: 0.285)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 71.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss = 0.059 (acc: 0.278), validation loss = 0.059 (acc: 0.290)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 71.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: train loss = 0.059 (acc: 0.279), validation loss = 0.060 (acc: 0.288)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:02<00:00, 73.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: train loss = 0.059 (acc: 0.280), validation loss = 0.060 (acc: 0.286)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:02<00:00, 73.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: train loss = 0.059 (acc: 0.279), validation loss = 0.060 (acc: 0.285)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:02<00:00, 72.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: train loss = 0.059 (acc: 0.280), validation loss = 0.060 (acc: 0.285)\n"
     ]
    }
   ],
   "source": [
    "mlp = ClassificationMLP(n_inputs=100).cuda()\n",
    "optimizerMLP = torch.optim.Adam(params=mlp.parameters(), lr=0.01)\n",
    "fit(model=mlp,\n",
    "    optimiser=optimizerMLP,\n",
    "    loss_fn=criterion,\n",
    "    train_dl=train_dl,\n",
    "    val_dl=val_dl,\n",
    "    epochs=15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "train_ds_sg = ClassificationDataset(train['text'].to_numpy(), train['label'].to_numpy(), '../ckpt/joined/sg_100/sg_100.kv')\n",
    "dev_ds_sg = ClassificationDataset(dev['text'].to_numpy(), dev['label'].to_numpy(), '../ckpt/joined/sg_100/sg_100.kv')\n",
    "test_ds_sg = ClassificationDataset(test['text'].to_numpy(), test['label'].to_numpy(), '../ckpt/joined/sg_100/sg_100.kv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "train_dl_sg = DataLoader(train_ds_sg, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl_sg = DataLoader(dev_ds_sg, batch_size=64, shuffle=True, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1476/13863 [00:05<00:47, 259.76it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [31]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m mlp_sg \u001B[38;5;241m=\u001B[39m ClassificationMLP(n_inputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m      2\u001B[0m optimizerMLP \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(params\u001B[38;5;241m=\u001B[39mmlp_sg\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m \u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmlp_sg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizerMLP\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_ds_sg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_dl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dl_sg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mfit\u001B[1;34m(model, optimiser, loss_fn, train_dl, val_dl, epochs, print_metrics)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m     39\u001B[0m     model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m---> 40\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m X_batch, y_batch \u001B[38;5;129;01min\u001B[39;00m tqdm(train_dl):\n\u001B[0;32m     41\u001B[0m         y_pred \u001B[38;5;241m=\u001B[39m model(X_batch)\n\u001B[0;32m     42\u001B[0m         loss \u001B[38;5;241m=\u001B[39m loss_fn(y_pred, y_batch\u001B[38;5;241m.\u001B[39mcuda())\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tqdm\\std.py:1195\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1192\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1194\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1195\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1196\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1197\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1198\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "Input \u001B[1;32mIn [16]\u001B[0m, in \u001B[0;36mClassificationDataset.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m     31\u001B[0m label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels[index]\n\u001B[0;32m     32\u001B[0m label \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(label, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[1;32m---> 33\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentence\u001B[49m\u001B[43m)\u001B[49m, label\n",
      "Input \u001B[1;32mIn [16]\u001B[0m, in \u001B[0;36mClassificationDataset.embed\u001B[1;34m(self, sentence)\u001B[0m\n\u001B[0;32m     25\u001B[0m tokens \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mLongTensor(tokens)\n\u001B[0;32m     26\u001B[0m embedded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(tokens)\n\u001B[1;32m---> 27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43membedded\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "mlp_sg = ClassificationMLP(n_inputs=100).cuda()\n",
    "optimizerMLP = torch.optim.Adam(params=mlp_sg.parameters(), lr=0.01)\n",
    "fit(model=mlp_sg,\n",
    "    optimiser=optimizerMLP,\n",
    "    loss_fn=criterion,\n",
    "    train_dl=train_ds_sg,\n",
    "    val_dl=val_dl_sg,\n",
    "    epochs=15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "train_ds_pr = ClassificationDataset(texts=train['text'].to_numpy(),\n",
    "                                    labels=train['label'].to_numpy(),\n",
    "                                    model_path='../ckpt/pretrained/word2vec_100_3_polish.bin')\n",
    "dev_ds_pr = ClassificationDataset(texts=train['text'].to_numpy(),\n",
    "                                  labels=train['label'].to_numpy(),\n",
    "                                  model_path='../ckpt/pretrained/word2vec_100_3_polish.bin')\n",
    "test_ds_pr = ClassificationDataset(texts=train['text'].to_numpy(),\n",
    "                                   labels=train['label'].to_numpy(),\n",
    "                                   model_path='../ckpt/pretrained/word2vec_100_3_polish.bin')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "train_dl_pr = DataLoader(train_ds_pr, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl_pr = DataLoader(dev_ds_pr, batch_size=64, shuffle=True, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 63.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 0.051 (acc: 0.276), validation loss = 0.051 (acc: 0.278)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 66.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 0.051 (acc: 0.280), validation loss = 0.051 (acc: 0.281)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 66.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train loss = 0.051 (acc: 0.280), validation loss = 0.051 (acc: 0.280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 67.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train loss = 0.051 (acc: 0.281), validation loss = 0.051 (acc: 0.279)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 70.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train loss = 0.051 (acc: 0.280), validation loss = 0.051 (acc: 0.280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 69.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train loss = 0.051 (acc: 0.281), validation loss = 0.051 (acc: 0.280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 63.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train loss = 0.051 (acc: 0.280), validation loss = 0.051 (acc: 0.280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 67.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train loss = 0.051 (acc: 0.281), validation loss = 0.051 (acc: 0.280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 70.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train loss = 0.051 (acc: 0.281), validation loss = 0.051 (acc: 0.281)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 67.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train loss = 0.051 (acc: 0.281), validation loss = 0.051 (acc: 0.280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 68.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss = 0.051 (acc: 0.281), validation loss = 0.051 (acc: 0.281)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 68.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: train loss = 0.051 (acc: 0.280), validation loss = 0.051 (acc: 0.280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 68.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: train loss = 0.051 (acc: 0.281), validation loss = 0.051 (acc: 0.281)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 69.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: train loss = 0.051 (acc: 0.281), validation loss = 0.051 (acc: 0.281)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:03<00:00, 67.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: train loss = 0.051 (acc: 0.281), validation loss = 0.051 (acc: 0.280)\n"
     ]
    }
   ],
   "source": [
    "mlp_pr = ClassificationMLP(n_inputs=100).cuda()\n",
    "optimizerMLP = torch.optim.Adam(params=mlp_pr.parameters(), lr=0.01)\n",
    "fit(model=mlp_pr,\n",
    "    optimiser=optimizerMLP,\n",
    "    loss_fn=criterion,\n",
    "    train_dl=train_dl_pr,\n",
    "    val_dl=val_dl_pr,\n",
    "    epochs=15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "Embedding(1929043, 100)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = joined_cbow_model.vectors\n",
    "embeddings = torch.Tensor(embeddings)\n",
    "emb = torch.nn.Embedding.from_pretrained(embeddings)\n",
    "emb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "Embedding(1934030, 100)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_pr_model = KeyedVectors.load('../ckpt/pretrained/word2vec_100_3_polish.bin')\n",
    "embeddings_pr = embeddings_pr_model.vectors\n",
    "embeddings_pr = torch.Tensor(embeddings_pr)\n",
    "emb_pr = torch.nn.Embedding.from_pretrained(embeddings_pr)\n",
    "emb_pr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "Embedding(1929043, 100)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = joined_skipgram_model.vectors\n",
    "embeddings = torch.Tensor(embeddings)\n",
    "emb = torch.nn.Embedding.from_pretrained(embeddings)\n",
    "emb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking similar words:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\keyedvectors.py:850: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  przekonywujący -> przekonywujące (0.81), przekonywający (0.79), przekonywującym (0.79), przekonujący (0.77), oględny (0.76), przekonywującego (0.76), przekonywująca (0.73), nieprzekonujący (0.72)\n",
      "  złodzieji -> aprochowa (0.57), gliksiusza (0.57), wyłudzenie (0.55), roegnera (0.52), damiensa (0.52), konkolitanos (0.51), euniosa (0.51), razieli (0.51)\n",
      "  swetr -> gabardynowy (0.60), cajgowy (0.60), kamgarn (0.60), flanelowy (0.59), bezrękawnik (0.59), drelichowy (0.59), popelinowy (0.59), czechczery (0.59)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key 'ludzią' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[1;32mIn [39]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mprint_similar\u001B[49m\u001B[43m(\u001B[49m\u001B[43membeddings_pr_model\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36mprint_similar\u001B[1;34m(model)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mChecking similar words:\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprzekonywujący\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzłodzieji\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mswetr\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mludzią\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m     11\u001B[0m   most_similar \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m%.2f\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m (similar, dist)\n\u001B[1;32m---> 12\u001B[0m                            \u001B[38;5;28;01mfor\u001B[39;00m similar, dist \u001B[38;5;129;01min\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmost_similar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m)\u001B[49m[:\u001B[38;5;241m8\u001B[39m])\n\u001B[0;32m     13\u001B[0m   \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m  \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m -> \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m (word, most_similar))\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\keyedvectors.py:842\u001B[0m, in \u001B[0;36mKeyedVectors.most_similar\u001B[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001B[0m\n\u001B[0;32m    839\u001B[0m         weight[idx] \u001B[38;5;241m=\u001B[39m item[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    841\u001B[0m \u001B[38;5;66;03m# compute the weighted average of all keys\u001B[39;00m\n\u001B[1;32m--> 842\u001B[0m mean \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_mean_vector\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpre_normalize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpost_normalize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_missing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    843\u001B[0m all_keys \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    844\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_index(key) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m keys \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, _KEY_TYPES) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhas_index_for(key)\n\u001B[0;32m    845\u001B[0m ]\n\u001B[0;32m    847\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m indexer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(topn, \u001B[38;5;28mint\u001B[39m):\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\keyedvectors.py:519\u001B[0m, in \u001B[0;36mKeyedVectors.get_mean_vector\u001B[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001B[0m\n\u001B[0;32m    517\u001B[0m         total_weight \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mabs\u001B[39m(weights[idx])\n\u001B[0;32m    518\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ignore_missing:\n\u001B[1;32m--> 519\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKey \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m not present in vocabulary\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    521\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m(total_weight \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m    522\u001B[0m     mean \u001B[38;5;241m=\u001B[39m mean \u001B[38;5;241m/\u001B[39m total_weight\n",
      "\u001B[1;31mKeyError\u001B[0m: \"Key 'ludzią' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "print_similar(embeddings_pr_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}