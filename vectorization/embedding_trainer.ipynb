{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corpora_path = \"data/ready_data\"\n",
    "\n",
    "def read_to_list(path_to_data):\n",
    "    data_df = pd.read_csv(path_to_data)\n",
    "    return data_df['text'].to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.replace('[', '')\n",
    "    text = text.replace(']', '')\n",
    "    text = text.replace('\\'', '')\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def make_uncased(corpus: List[List[str]]) -> List[List[str]]:\n",
    "    result = []\n",
    "    for sentence in corpus:\n",
    "        uncased = []\n",
    "        for word in sentence:\n",
    "            uncased.append(word.lower())\n",
    "        result.append(uncased)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['najmłodsi', 'obchody', 'swojego', 'święta', 'w', 'tym', 'roku', 'rozpoczęli', 'w', 'sobotę', 'odbyło', 'się', 'wiele', 'imprez', 'dla', 'najmłodszych', 'festynów', 'zabaw', 'dopisała', 'pogoda', 'sponsorzy', 'lokalnych', 'imprez', 'zapewnili', 'dzieciom', 'słodycze', 'i', 'zabawę', 'sobotni', 'festyn', 'na', 'ul', 'monte', 'cassino', 'w', 'sopocie', 'zorganizowany', 'przez', 'fundację', 'rodzina', 'nadziei', 'w', 'tym', 'roku', 'nosił', 'nazwę', 'ulica', 'walecznych', 'serc'], ['konserwator', 'zabytków', 'ustalił', 'szkody', 'jakie', 'w', 'jednej', 'z', 'komnat', 'zamku', 'królewskiego', 'na', 'wawelu', 'spowodował', 'nietrzeźwy', 'turysta', 'z', 'województwa', 'suwalskiego', 'mężczyzna', 'wskoczył', 'do', 'łoża', 'zygmunta', 'starego', 'i', 'uszkodził', 'przykrywającą', 'je', 'kapę', 'zdarzenie', 'miało', 'miejsce', 'w', 'minioną', 'sobotę', '36letni', 'robert', 'j', 'w', 'towarzystwie', 'kilkuosobowej', 'grupy', 'znajomych', 'zwiedzał', 'zamek', 'królewski', 'na', 'wawelu']]\n"
     ]
    }
   ],
   "source": [
    "corp = [['Najmłodsi', 'obchody', 'swojego', 'święta', 'w', 'tym', 'roku', 'rozpoczęli', 'w', 'sobotę', 'Odbyło', 'się', 'wiele', 'imprez', 'dla', 'najmłodszych', 'festynów', 'zabaw', 'Dopisała', 'pogoda', 'Sponsorzy', 'lokalnych', 'imprez', 'zapewnili', 'dzieciom', 'słodycze', 'i', 'zabawę', 'Sobotni', 'festyn', 'na', 'ul', 'Monte', 'Cassino', 'w', 'Sopocie', 'zorganizowany', 'przez', 'Fundację', 'Rodzina', 'Nadziei', 'w', 'tym', 'roku', 'nosił', 'nazwę', 'Ulica', 'Walecznych', 'Serc'], ['Konserwator', 'zabytków', 'ustalił', 'szkody', 'jakie', 'w', 'jednej', 'z', 'komnat', 'Zamku', 'Królewskiego', 'na', 'Wawelu', 'spowodował', 'nietrzeźwy', 'turysta', 'z', 'województwa', 'suwalskiego', 'Mężczyzna', 'wskoczył', 'do', 'łoża', 'Zygmunta', 'Starego', 'i', 'uszkodził', 'przykrywającą', 'je', 'kapę', 'Zdarzenie', 'miało', 'miejsce', 'w', 'minioną', 'sobotę', '36letni', 'Robert', 'J', 'w', 'towarzystwie', 'kilkuosobowej', 'grupy', 'znajomych', 'zwiedzał', 'Zamek', 'Królewski', 'na', 'Wawelu']]\n",
    "\n",
    "print(make_uncased(corp))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def prepare_corpus(is_joined: bool, is_full: bool):\n",
    "    full_name = r\"C:\\SI22_2\\NLP\\dataset\\stylometry\\data\\ready_data\\full_corpus.csv\"\n",
    "    special_name = r\"C:\\SI22_2\\NLP\\dataset\\stylometry\\data\\ready_data\\tweets_pl.csv\"\n",
    "    fc = read_to_list(full_name)\n",
    "    fc = [clean_text(line) for line in fc]\n",
    "    sc = read_to_list(special_name)\n",
    "    sc = [clean_text(line) for line in sc]\n",
    "    if is_joined:\n",
    "        fc.extend(sc)\n",
    "        fc = [word_tokenize(text=line, language='polish') for line in fc]\n",
    "        return make_uncased(fc)\n",
    "    else:\n",
    "        if is_full:\n",
    "            fc = [word_tokenize(text=line, language='polish') for line in fc]\n",
    "            return make_uncased(fc)\n",
    "        else:\n",
    "            sc = [word_tokenize(text=line, language='polish') for line in sc]\n",
    "            return make_uncased(sc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018683\n",
      "[['najmłodsi', 'obchody', 'swojego', 'święta', 'w', 'tym', 'roku', 'rozpoczęli', 'w', 'sobotę', 'odbyło', 'się', 'wiele', 'imprez', 'dla', 'najmłodszych', 'festynów', 'zabaw', 'dopisała', 'pogoda', 'sponsorzy', 'lokalnych', 'imprez', 'zapewnili', 'dzieciom', 'słodycze', 'i', 'zabawę', 'sobotni', 'festyn', 'na', 'ul', 'monte', 'cassino', 'w', 'sopocie', 'zorganizowany', 'przez', 'fundację', 'rodzina', 'nadziei', 'w', 'tym', 'roku', 'nosił', 'nazwę', 'ulica', 'walecznych', 'serc'], ['konserwator', 'zabytków', 'ustalił', 'szkody', 'jakie', 'w', 'jednej', 'z', 'komnat', 'zamku', 'królewskiego', 'na', 'wawelu', 'spowodował', 'nietrzeźwy', 'turysta', 'z', 'województwa', 'suwalskiego', 'mężczyzna', 'wskoczył', 'do', 'łoża', 'zygmunta', 'starego', 'i', 'uszkodził', 'przykrywającą', 'je', 'kapę', 'zdarzenie', 'miało', 'miejsce', 'w', 'minioną', 'sobotę', '36letni', 'robert', 'j', 'w', 'towarzystwie', 'kilkuosobowej', 'grupy', 'znajomych', 'zwiedzał', 'zamek', 'królewski', 'na', 'wawelu'], ['–', 'musi', 'wyjść', '–', 'a', 'na', 'razie', 'to', 'samochodzik', 'bym', 'chciał', 'jednak', 'kupić', 'bo', 'to', 'to', 'takie', 'wiesz', 'kurde', 'no', 'w', 'sumie', 'mógłbym', 'sobie', 'kupić', 'czinkłeczento', 'na', 'przykład', 'nie', 'jeździ', '–', 'znaczy', 'nawet', 'mógłbyś', 'sobie', 'kupić', 'za', 'tą', 'kwotę', 'sejczento', 'i', 'to', 'nie', 'stare', 'bo', 'półtora', 'roczne', 'i', 'to', 'też', 'jeździ', '–', 'ja', 'myślałem', 'o', 'sejczento', 'nawet', 'wiesz', 'tylko', 'tak'], ['czekam', 'na', 'wańkowicza', 'który', 'zadzwonił', 'przed', 'ósmą', 'rano', 'że', 'będzie', 'o', 'wpół', 'do', 'dziewiątej', 'a', 'dochodzi', 'w', 'pół', 'do', 'dziesiątej', 'sprawiło', 'to', 'że', 'się', 'właśnie', 'wytuszowałam', 'i', 'ubrałam', 'przeglądam', 'dziennik', 'swój', 'jest', 'bardzo', 'prawdziwy', 'szczery', 'tym', 'ważniejsze', 'jest', 'pisanie', 'że', 'mniej', 'się', 'pamięta', 'kiedy', 'tyle', 'się', 'dzieje'], ['czeski', 'parlament', 'uchwalił', 'ustawę', 'o', 'zaostrzeniu', 'odpowiedzialności', 'karnej', 'za', 'przechowywanie', 'rozpowszechnianie', 'i', 'produkcję', 'narkotyków', 'ustawa', 'przewiduje', 'karę', 'więzienia', 'za', 'przechowywanie', 'określonej', 'ilości', 'narkotyków', 'ilość', 'tę', 'określać', 'będzie', 'policja', 'która', 'otrzymała', 'prawo', 'zatrzymywania', 'osób', 'winnych', 'naruszania', 'ustawy', 'i', 'oddawania', 'ich', 'pod', 'sąd', 'narkotyki', 'stają', 'się', 'coraz', 'większą', 'plagą', 'wśród', 'czeskiej', 'młodzieży']]\n"
     ]
    }
   ],
   "source": [
    "joined = prepare_corpus(is_joined=True, is_full=False)\n",
    "print(len(joined))\n",
    "print(joined[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "817480\n",
      "[['Najmłodsi', 'obchody', 'swojego', 'święta', 'w', 'tym', 'roku', 'rozpoczęli', 'w', 'sobotę', 'Odbyło', 'się', 'wiele', 'imprez', 'dla', 'najmłodszych', 'festynów', 'zabaw', 'Dopisała', 'pogoda', 'Sponsorzy', 'lokalnych', 'imprez', 'zapewnili', 'dzieciom', 'słodycze', 'i', 'zabawę', 'Sobotni', 'festyn', 'na', 'ul', 'Monte', 'Cassino', 'w', 'Sopocie', 'zorganizowany', 'przez', 'Fundację', 'Rodzina', 'Nadziei', 'w', 'tym', 'roku', 'nosił', 'nazwę', 'Ulica', 'Walecznych', 'Serc'], ['Konserwator', 'zabytków', 'ustalił', 'szkody', 'jakie', 'w', 'jednej', 'z', 'komnat', 'Zamku', 'Królewskiego', 'na', 'Wawelu', 'spowodował', 'nietrzeźwy', 'turysta', 'z', 'województwa', 'suwalskiego', 'Mężczyzna', 'wskoczył', 'do', 'łoża', 'Zygmunta', 'Starego', 'i', 'uszkodził', 'przykrywającą', 'je', 'kapę', 'Zdarzenie', 'miało', 'miejsce', 'w', 'minioną', 'sobotę', '36letni', 'Robert', 'J', 'w', 'towarzystwie', 'kilkuosobowej', 'grupy', 'znajomych', 'zwiedzał', 'Zamek', 'Królewski', 'na', 'Wawelu'], ['–', 'musi', 'wyjść', '–', 'a', 'na', 'razie', 'to', 'samochodzik', 'bym', 'chciał', 'jednak', 'kupić', 'bo', 'to', 'to', 'takie', 'wiesz', 'kurde', 'no', 'w', 'sumie', 'mógłbym', 'sobie', 'kupić', 'czinkłeczento', 'na', 'przykład', 'nie', 'jeździ', '–', 'znaczy', 'nawet', 'mógłbyś', 'sobie', 'kupić', 'za', 'tą', 'kwotę', 'sejczento', 'i', 'to', 'nie', 'stare', 'bo', 'półtora', 'roczne', 'i', 'to', 'też', 'jeździ', '–', 'ja', 'myślałem', 'o', 'sejczento', 'nawet', 'wiesz', 'tylko', 'tak'], ['Czekam', 'na', 'Wańkowicza', 'który', 'zadzwonił', 'przed', 'ósmą', 'rano', 'że', 'będzie', 'o', 'wpół', 'do', 'dziewiątej', 'a', 'dochodzi', 'w', 'pół', 'do', 'dziesiątej', 'Sprawiło', 'to', 'że', 'się', 'właśnie', 'wytuszowałam', 'i', 'ubrałam', 'Przeglądam', 'dziennik', 'swój', 'Jest', 'bardzo', 'prawdziwy', 'szczery', 'Tym', 'ważniejsze', 'jest', 'pisanie', 'że', 'mniej', 'się', 'pamięta', 'kiedy', 'tyle', 'się', 'dzieje'], ['Czeski', 'parlament', 'uchwalił', 'ustawę', 'o', 'zaostrzeniu', 'odpowiedzialności', 'karnej', 'za', 'przechowywanie', 'rozpowszechnianie', 'i', 'produkcję', 'narkotyków', 'Ustawa', 'przewiduje', 'karę', 'więzienia', 'za', 'przechowywanie', 'określonej', 'ilości', 'narkotyków', 'Ilość', 'tę', 'określać', 'będzie', 'policja', 'która', 'otrzymała', 'prawo', 'zatrzymywania', 'osób', 'winnych', 'naruszania', 'ustawy', 'i', 'oddawania', 'ich', 'pod', 'sąd', 'Narkotyki', 'stają', 'się', 'coraz', 'większą', 'plagą', 'wśród', 'czeskiej', 'młodzieży']]\n"
     ]
    }
   ],
   "source": [
    "full = prepare_corpus(is_joined=False, is_full=True)\n",
    "print(len(full))\n",
    "print(full[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "special = prepare_corpus(is_joined=False, is_full=False)\n",
    "print(len(special))\n",
    "print(special[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import Counter\n",
    "\n",
    "def get_stats(corpus: List[List[str]]) -> pd.DataFrame:\n",
    "    len_count = []\n",
    "    for l in corpus:\n",
    "        len_count.append(len(l))\n",
    "    return pd.DataFrame(sorted(Counter(len_count).items()), columns=[\"No of Words in each Sentence\",\"No of sentence\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "joined_sent_df = get_stats(joined)\n",
    "joined_sent_df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_sent_df = get_stats(full)\n",
    "full_sent_df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "special_sent_df = get_stats(special)\n",
    "special_sent_df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def train_word2vec(vector_size: int,\n",
    "                   sg: bool,\n",
    "                   tokenized_ds: List[List[str]],\n",
    "                   is_full_c: bool = False,\n",
    "                   is_joined: bool = False) -> None:\n",
    "    model =  Word2Vec(sentences=tokenized_ds, vector_size=vector_size, window=3, min_count=1, workers=4, sg=int(sg))\n",
    "    if is_joined:\n",
    "        ckpt_path = r'C:\\SI22_2\\NLP\\dataset\\stylometry\\ckpt\\joined'\n",
    "    else:\n",
    "        if is_full_c:\n",
    "            ckpt_path = r'C:\\SI22_2\\NLP\\dataset\\stylometry\\ckpt\\full'\n",
    "        else:\n",
    "            ckpt_path = r'C:\\SI22_2\\NLP\\dataset\\stylometry\\ckpt\\special'\n",
    "    if sg:\n",
    "        model_dir = '_'.join(['sg', str(vector_size)])\n",
    "        model_name = '_'.join([model_dir, 'model'])\n",
    "    else:\n",
    "        model_dir = '_'.join(['cbow', str(vector_size)])\n",
    "        model_name = '_'.join([model_dir, 'model'])\n",
    "    dir_path = os.path.join(ckpt_path, model_dir)\n",
    "    os.makedirs(dir_path)\n",
    "    full_path = os.path.join(dir_path, model_name)\n",
    "    model.save(full_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_word2vec(vector_size=100, sg=True, tokenized_ds=special)\n",
    "train_word2vec(vector_size=300, sg=True, tokenized_ds=special)\n",
    "train_word2vec(vector_size=100, sg=False, tokenized_ds=special)\n",
    "train_word2vec(vector_size=300, sg=False, tokenized_ds=special)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "train_word2vec(vector_size=100, sg=True, tokenized_ds=joined, is_joined=True)\n",
    "# train_word2vec(vector_size=300, sg=True, tokenized_ds=joined, is_joined=True)\n",
    "train_word2vec(vector_size=100, sg=False, tokenized_ds=joined, is_joined=True)\n",
    "# train_word2vec(vector_size=300, sg=False, tokenized_ds=joined, is_joined=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_word2vec(vector_size=100, sg=True, tokenized_ds=full, is_full_c=True)\n",
    "# train_word2vec(vector_size=300, sg=True, tokenized_ds=full, is_full_c=True)\n",
    "train_word2vec(vector_size=100, sg=False, tokenized_ds=full, is_full_c=True)\n",
    "# train_word2vec(vector_size=300, sg=False, tokenized_ds=full, is_full_c=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "def train_fasttext(vector_size: int, tokenized_ds: List[List[str]], is_full_c: bool = False, is_joined: bool = False):\n",
    "    model =  FastText(sentences=tokenized_ds, vector_size=vector_size, window=3, min_count=1, workers=4)\n",
    "    if is_joined:\n",
    "        ckpt_path = r'C:\\SI22_2\\NLP\\dataset\\stylometry\\ckpt\\joined'\n",
    "    else:\n",
    "        if is_full_c:\n",
    "            ckpt_path = r'C:\\SI22_2\\NLP\\dataset\\stylometry\\ckpt\\full'\n",
    "        else:\n",
    "            ckpt_path = r'C:\\SI22_2\\NLP\\dataset\\stylometry\\ckpt\\special'\n",
    "    model_dir = '_'.join(['fasttext', str(vector_size)])\n",
    "    model_name = '_'.join([model_dir, 'model'])\n",
    "    dir_path = os.path.join(ckpt_path, model_dir)\n",
    "    os.makedirs(dir_path)\n",
    "    full_path = os.path.join(dir_path, model_name)\n",
    "    model.save(full_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_fasttext(vector_size=100, tokenized_ds=special, is_full_c=False)\n",
    "# train_fasttext(vector_size=300, tokenized_ds=special, is_full_c=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain_fasttext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvector_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenized_ds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfull\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_full_c\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36mtrain_fasttext\u001B[1;34m(vector_size, tokenized_ds, is_full_c, is_joined)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_fasttext\u001B[39m(vector_size: \u001B[38;5;28mint\u001B[39m, tokenized_ds: List[List[\u001B[38;5;28mstr\u001B[39m]], is_full_c: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, is_joined: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m----> 4\u001B[0m     model \u001B[38;5;241m=\u001B[39m  \u001B[43mFastText\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenized_ds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvector_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvector_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_count\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_joined:\n\u001B[0;32m      6\u001B[0m         ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mSI22_2\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mNLP\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mdataset\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mstylometry\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mckpt\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mjoined\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\fasttext.py:435\u001B[0m, in \u001B[0;36mFastText.__init__\u001B[1;34m(self, sentences, corpus_file, sg, hs, vector_size, alpha, window, min_count, max_vocab_size, word_ngrams, sample, seed, workers, min_alpha, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, min_n, max_n, sorted_vocab, bucket, trim_rule, batch_words, callbacks, max_final_vocab, shrink_windows)\u001B[0m\n\u001B[0;32m    432\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwv\u001B[38;5;241m.\u001B[39mvectors_vocab_lockf \u001B[38;5;241m=\u001B[39m ones(\u001B[38;5;241m1\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mREAL)\n\u001B[0;32m    433\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwv\u001B[38;5;241m.\u001B[39mvectors_ngrams_lockf \u001B[38;5;241m=\u001B[39m ones(\u001B[38;5;241m1\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mREAL)\n\u001B[1;32m--> 435\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mFastText\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    436\u001B[0m \u001B[43m    \u001B[49m\u001B[43msentences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msentences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcorpus_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvector_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvector_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    437\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_words\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrim_rule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrim_rule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    438\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_vocab_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_vocab_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_final_vocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_final_vocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    439\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmin_count\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmin_count\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msorted_vocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msorted_vocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    440\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnull_word\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnull_word\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mns_exponent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mns_exponent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhashfxn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhashfxn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    441\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnegative\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnegative\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcbow_mean\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcbow_mean\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    442\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmin_alpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmin_alpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshrink_windows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshrink_windows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\word2vec.py:426\u001B[0m, in \u001B[0;36mWord2Vec.__init__\u001B[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001B[0m\n\u001B[0;32m    424\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_iterable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m corpus_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    425\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_corpus_sanity(corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, passes\u001B[38;5;241m=\u001B[39m(epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m--> 426\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus_iterable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_iterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcorpus_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrim_rule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrim_rule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    427\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain(\n\u001B[0;32m    428\u001B[0m         corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, total_examples\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_count,\n\u001B[0;32m    429\u001B[0m         total_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_total_words, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepochs, start_alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malpha,\n\u001B[0;32m    430\u001B[0m         end_alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_alpha, compute_loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss, callbacks\u001B[38;5;241m=\u001B[39mcallbacks)\n\u001B[0;32m    431\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\word2vec.py:492\u001B[0m, in \u001B[0;36mWord2Vec.build_vocab\u001B[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001B[0m\n\u001B[0;32m    490\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_count \u001B[38;5;241m=\u001B[39m corpus_count\n\u001B[0;32m    491\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_total_words \u001B[38;5;241m=\u001B[39m total_words\n\u001B[1;32m--> 492\u001B[0m report_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mupdate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeep_raw_vocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_raw_vocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrim_rule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrim_rule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    493\u001B[0m report_values[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmemory\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimate_memory(vocab_size\u001B[38;5;241m=\u001B[39mreport_values[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_retained_words\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m    494\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_weights(update\u001B[38;5;241m=\u001B[39mupdate)\n",
      "File \u001B[1;32mc:\\users\\test\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\word2vec.py:649\u001B[0m, in \u001B[0;36mWord2Vec.prepare_vocab\u001B[1;34m(self, update, keep_raw_vocab, trim_rule, min_count, sample, dry_run)\u001B[0m\n\u001B[0;32m    647\u001B[0m     retain_total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m v\n\u001B[0;32m    648\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m dry_run:\n\u001B[1;32m--> 649\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwv\u001B[38;5;241m.\u001B[39mkey_to_index[word] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwv\u001B[38;5;241m.\u001B[39mindex_to_key)\n\u001B[0;32m    650\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwv\u001B[38;5;241m.\u001B[39mindex_to_key\u001B[38;5;241m.\u001B[39mappend(word)\n\u001B[0;32m    651\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mMemoryError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_fasttext(vector_size=100, tokenized_ds=full, is_full_c=True)\n",
    "# train_fasttext(vector_size=300, tokenized_ds=full, is_full_c=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_fasttext(vector_size=100, tokenized_ds=joined, is_joined=True)\n",
    "# train_fasttext(vector_size=300, tokenized_ds=joined, is_joined=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}