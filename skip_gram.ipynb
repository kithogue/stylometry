{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "817480\n",
      "['[\\'Najmłodsi obchody swojego święta w tym roku rozpoczęli w sobotę. Odbyło się wiele imprez dla najmłodszych - festynów, zabaw. Dopisała pogoda. Sponsorzy lokalnych imprez zapewnili dzieciom słodycze i zabawę. Sobotni festyn na ul. Monte Cassino w Sopocie, zorganizowany przez Fundację Rodzina Nadziei, w tym roku nosił nazwę \"Ulica Walecznych Serc\".\\']', \"['Konserwator zabytków ustalił szkody, jakie w jednej z komnat Zamku Królewskiego na Wawelu spowodował nietrzeźwy turysta z województwa suwalskiego. Mężczyzna wskoczył do łoża Zygmunta Starego i uszkodził przykrywającą je kapę. Zdarzenie miało miejsce w minioną sobotę. 36-letni Robert J. w towarzystwie kilkuosobowej grupy znajomych zwiedzał Zamek Królewski na Wawelu.']\", \"['– musi wyjść..', '– a na razie to samochodzik bym chciał jednak kupić bo to to takie wiesz kurde no w sumie mógłbym sobie kupić czinkłeczento na przykład nie? jeździ..', '– znaczy nawet mógłbyś sobie kupić za tą kwotę sejczento i to nie stare bo półtora roczne. i to też jeździ..', '– ja myślałem o sejczento nawet wiesz tylko tak..']\", \"['Czekam na Wańkowicza, który zadzwonił przed ósmą rano, że będzie o wpół do dziewiątej, a dochodzi w pół do dziesiątej. Sprawiło to, że się właśnie wytuszowałam i ubrałam.', 'Przeglądam dziennik swój. Jest bardzo prawdziwy, szczery. Tym ważniejsze jest pisanie, że mniej się pamięta, kiedy tyle się dzieje.']\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "full_corpus_path = \"../data/full_corpus/full_corpus.csv\"\n",
    "\n",
    "def read_to_list(path):\n",
    "    data_df = pd.read_csv(path)\n",
    "    return data_df['text'].to_list()\n",
    "\n",
    "fc = read_to_list(full_corpus_path)\n",
    "\n",
    "print(len(fc))\n",
    "print(fc[:4])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.replace('[', '')\n",
    "    text = text.replace(']', '')\n",
    "    return text.replace('\\'', '')\n",
    "\n",
    "fc = [clean_text(line) for line in fc]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ndazhunts/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[['Najmłodsi',\n  'obchody',\n  'swojego',\n  'święta',\n  'w',\n  'tym',\n  'roku',\n  'rozpoczęli',\n  'w',\n  'sobotę',\n  '.',\n  'Odbyło',\n  'się',\n  'wiele',\n  'imprez',\n  'dla',\n  'najmłodszych',\n  '-',\n  'festynów',\n  ',',\n  'zabaw',\n  '.',\n  'Dopisała',\n  'pogoda',\n  '.',\n  'Sponsorzy',\n  'lokalnych',\n  'imprez',\n  'zapewnili',\n  'dzieciom',\n  'słodycze',\n  'i',\n  'zabawę',\n  '.',\n  'Sobotni',\n  'festyn',\n  'na',\n  'ul.',\n  'Monte',\n  'Cassino',\n  'w',\n  'Sopocie',\n  ',',\n  'zorganizowany',\n  'przez',\n  'Fundację',\n  'Rodzina',\n  'Nadziei',\n  ',',\n  'w',\n  'tym',\n  'roku',\n  'nosił',\n  'nazwę',\n  '``',\n  'Ulica',\n  'Walecznych',\n  'Serc',\n  \"''\",\n  '.'],\n ['Konserwator',\n  'zabytków',\n  'ustalił',\n  'szkody',\n  ',',\n  'jakie',\n  'w',\n  'jednej',\n  'z',\n  'komnat',\n  'Zamku',\n  'Królewskiego',\n  'na',\n  'Wawelu',\n  'spowodował',\n  'nietrzeźwy',\n  'turysta',\n  'z',\n  'województwa',\n  'suwalskiego',\n  '.',\n  'Mężczyzna',\n  'wskoczył',\n  'do',\n  'łoża',\n  'Zygmunta',\n  'Starego',\n  'i',\n  'uszkodził',\n  'przykrywającą',\n  'je',\n  'kapę',\n  '.',\n  'Zdarzenie',\n  'miało',\n  'miejsce',\n  'w',\n  'minioną',\n  'sobotę',\n  '.',\n  '36-letni',\n  'Robert',\n  'J.',\n  'w',\n  'towarzystwie',\n  'kilkuosobowej',\n  'grupy',\n  'znajomych',\n  'zwiedzał',\n  'Zamek',\n  'Królewski',\n  'na',\n  'Wawelu',\n  '.'],\n ['–',\n  'musi',\n  'wyjść',\n  '..',\n  ',',\n  '–',\n  'a',\n  'na',\n  'razie',\n  'to',\n  'samochodzik',\n  'bym',\n  'chciał',\n  'jednak',\n  'kupić',\n  'bo',\n  'to',\n  'to',\n  'takie',\n  'wiesz',\n  'kurde',\n  'no',\n  'w',\n  'sumie',\n  'mógłbym',\n  'sobie',\n  'kupić',\n  'czinkłeczento',\n  'na',\n  'przykład',\n  'nie',\n  '?',\n  'jeździ',\n  '..',\n  ',',\n  '–',\n  'znaczy',\n  'nawet',\n  'mógłbyś',\n  'sobie',\n  'kupić',\n  'za',\n  'tą',\n  'kwotę',\n  'sejczento',\n  'i',\n  'to',\n  'nie',\n  'stare',\n  'bo',\n  'półtora',\n  'roczne',\n  '.',\n  'i',\n  'to',\n  'też',\n  'jeździ',\n  '..',\n  ',',\n  '–',\n  'ja',\n  'myślałem',\n  'o',\n  'sejczento',\n  'nawet',\n  'wiesz',\n  'tylko',\n  'tak',\n  '..'],\n ['Czekam',\n  'na',\n  'Wańkowicza',\n  ',',\n  'który',\n  'zadzwonił',\n  'przed',\n  'ósmą',\n  'rano',\n  ',',\n  'że',\n  'będzie',\n  'o',\n  'wpół',\n  'do',\n  'dziewiątej',\n  ',',\n  'a',\n  'dochodzi',\n  'w',\n  'pół',\n  'do',\n  'dziesiątej',\n  '.',\n  'Sprawiło',\n  'to',\n  ',',\n  'że',\n  'się',\n  'właśnie',\n  'wytuszowałam',\n  'i',\n  'ubrałam.',\n  ',',\n  'Przeglądam',\n  'dziennik',\n  'swój',\n  '.',\n  'Jest',\n  'bardzo',\n  'prawdziwy',\n  ',',\n  'szczery',\n  '.',\n  'Tym',\n  'ważniejsze',\n  'jest',\n  'pisanie',\n  ',',\n  'że',\n  'mniej',\n  'się',\n  'pamięta',\n  ',',\n  'kiedy',\n  'tyle',\n  'się',\n  'dzieje',\n  '.']]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "fc_tokenized = [word_tokenize(text=line, language='polish') for line in fc]\n",
    "fc_tokenized[:4]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['Azikiwe', 'RedHawks', 'Boitsov', 'Siarnicki']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def flatten(nested_list: List[List[str]]) -> List[str]:\n",
    "    return [item for sublist in nested_list for item in sublist]\n",
    "\n",
    "vocab = list(set(flatten(fc_tokenized)))\n",
    "vocab[:4]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "2163500"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def get_word2idx(vocabulary: List[str]) -> Dict[str, int]:\n",
    "    return {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "def get_idx2word(vocabulary: List[str]) -> Dict[int, str]:\n",
    "    return {idx: w for (idx, w) in enumerate(vocabulary)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def get_idx_pairs(tokenized_corpus: List[List[str]]) -> np.ndarray:\n",
    "    window_size = 2\n",
    "    idx_pairs = []\n",
    "    word2idx = get_word2idx(vocabulary=vocab)\n",
    "    # for each sentence\n",
    "    for sentence in tokenized_corpus:\n",
    "        indices = [word2idx[word] for word in sentence]\n",
    "        # for each word, treated as center word\n",
    "        for center_word_pos in range(len(indices)):\n",
    "            # for each window position\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                # make sure not to jump out sentence\n",
    "                if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                context_word_idx = indices[context_word_pos]\n",
    "                idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "    return np.array(idx_pairs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocab).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def train_embeddings(vocabulary_size: int, idx_pairs: np.ndarray) -> None:\n",
    "    embedding_dims = 5\n",
    "    W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "    W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "    num_epochs = 101\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    for epo in range(num_epochs):\n",
    "        loss_val = 0\n",
    "        for data, target in idx_pairs:\n",
    "            x = Variable(get_input_layer(data)).float()\n",
    "            y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "            z1 = torch.matmul(W1, x)\n",
    "            z2 = torch.matmul(W2, z1)\n",
    "\n",
    "            log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "            loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "            loss_val += loss.data[0]\n",
    "            loss.backward()\n",
    "            W1.data -= learning_rate * W1.grad.data\n",
    "            W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "            W1.grad.data.zero_()\n",
    "            W2.grad.data.zero_()\n",
    "        if epo % 10 == 0:\n",
    "            print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fc_idx_pairs = get_idx_pairs(tokenized_corpus=fc_tokenized)\n",
    "vocab_size = len(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_embeddings(vocabulary_size=vocab_size, idx_pairs=fc_idx_pairs)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
